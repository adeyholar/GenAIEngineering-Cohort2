{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive Machine Learning Analysis of the Iris Dataset\n",
    "\n",
    "## Table of Contents\n",
    "1. **Data Loading and Exploration**\n",
    "2. **Exploratory Data Analysis (EDA)**\n",
    "3. **K-Means Clustering Analysis**\n",
    "4. **Regression Analysis**\n",
    "5. **Classification with Multiple Algorithms**\n",
    "6. **Model Comparison and Conclusions**\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "The Iris dataset is a classic dataset in machine learning, containing measurements of 150 iris flowers from three species: Setosa, Versicolor, and Virginica. Each sample has four features:\n",
    "- Sepal Length (cm)\n",
    "- Sepal Width (cm)\n",
    "- Petal Length (cm)\n",
    "- Petal Width (cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import (accuracy_score, classification_report, confusion_matrix, \n",
    "                           silhouette_score, mean_squared_error, r2_score)\n",
    "\n",
    "# Clustering\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Regression\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Classification\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure visualization settings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "feature_names = iris.feature_names\n",
    "target_names = iris.target_names\n",
    "\n",
    "# Create a DataFrame for easier manipulation\n",
    "df = pd.DataFrame(X, columns=feature_names)\n",
    "df.to_csv('iris.csv', index=False)\n",
    "df['species'] = y\n",
    "df['species_name'] = df['species'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})\n",
    "\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis (EDA)\n",
    "\n",
    "Let's explore the dataset to understand its characteristics, distributions, and relationships between variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "print(\"Dataset Information:\")\n",
    "print(df.info())\n",
    "print(\"\\nStatistical Summary:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values:\")\n",
    "print(df.isnull().sum())\n",
    "print(\"\\nSpecies distribution:\")\n",
    "print(df['species_name'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution plots for each feature\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(feature_names):\n",
    "    axes[idx].hist(df[col], bins=20, edgecolor='black', alpha=0.7)\n",
    "    axes[idx].set_title(f'Distribution of {col}')\n",
    "    axes[idx].set_xlabel(col)\n",
    "    axes[idx].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots by species\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(feature_names):\n",
    "    df.boxplot(column=col, by='species_name', ax=axes[idx])\n",
    "    axes[idx].set_title(f'{col} by Species')\n",
    "    axes[idx].set_xlabel('Species')\n",
    "    axes[idx].set_ylabel(col)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "correlation_matrix = df[feature_names].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, linewidths=1)\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.show()\n",
    "\n",
    "print(\"Observations:\")\n",
    "print(\"- Petal length and petal width are highly correlated (0.96)\")\n",
    "print(\"- Sepal length also shows strong correlation with petal dimensions\")\n",
    "print(\"- Sepal width has the weakest correlations with other features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pair plot to visualize relationships\n",
    "plt.figure(figsize=(12, 10))\n",
    "pair_plot = sns.pairplot(df, hue='species_name', diag_kind='kde', \n",
    "                         markers=['o', 's', 'D'], palette='husl')\n",
    "pair_plot.fig.suptitle('Pairwise Relationships in Iris Dataset', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "print(\"Key Insights from Pair Plot:\")\n",
    "print(\"- Setosa is clearly separable from other species\")\n",
    "print(\"- Versicolor and Virginica show some overlap\")\n",
    "print(\"- Petal measurements provide better species separation than sepal measurements\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. K-Means Clustering Analysis\n",
    "\n",
    "K-means clustering is an unsupervised learning algorithm that groups data points into K clusters based on feature similarity. Let's apply it to the 4 numeric features of the Iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for clustering (using only numeric features)\n",
    "X_cluster = df[feature_names].values\n",
    "\n",
    "# Standardize the features (important for K-means)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_cluster)\n",
    "\n",
    "print(\"Shape of clustering data:\", X_scaled.shape)\n",
    "print(\"Features used:\", feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elbow method to find optimal number of clusters\n",
    "inertias = []\n",
    "silhouette_scores = []\n",
    "K_range = range(2, 11)\n",
    "\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_scaled)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(X_scaled, kmeans.labels_))\n",
    "\n",
    "# Plot elbow curve and silhouette scores\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Elbow plot\n",
    "ax1.plot(K_range, inertias, 'bo-')\n",
    "ax1.set_xlabel('Number of Clusters (k)')\n",
    "ax1.set_ylabel('Inertia')\n",
    "ax1.set_title('Elbow Method for Optimal k')\n",
    "ax1.grid(True)\n",
    "\n",
    "# Silhouette score plot\n",
    "ax2.plot(K_range, silhouette_scores, 'ro-')\n",
    "ax2.set_xlabel('Number of Clusters (k)')\n",
    "ax2.set_ylabel('Silhouette Score')\n",
    "ax2.set_title('Silhouette Score for Different k')\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Optimal k appears to be 3 based on the elbow method and silhouette scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply K-means with k=3\n",
    "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "cluster_labels = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Add cluster labels to dataframe\n",
    "df['cluster'] = cluster_labels\n",
    "\n",
    "# Compare clusters with actual species\n",
    "cluster_comparison = pd.crosstab(df['species_name'], df['cluster'])\n",
    "print(\"Cluster vs Species Comparison:\")\n",
    "print(cluster_comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize clustering results\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Reduce dimensions for visualization\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Create subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Plot 1: Clusters\n",
    "scatter1 = ax1.scatter(X_pca[:, 0], X_pca[:, 1], c=cluster_labels, cmap='viridis', s=50)\n",
    "ax1.set_xlabel('First Principal Component')\n",
    "ax1.set_ylabel('Second Principal Component')\n",
    "ax1.set_title('K-Means Clustering Results (k=3)')\n",
    "plt.colorbar(scatter1, ax=ax1)\n",
    "\n",
    "# Plot 2: Actual species\n",
    "scatter2 = ax2.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', s=50)\n",
    "ax2.set_xlabel('First Principal Component')\n",
    "ax2.set_ylabel('Second Principal Component')\n",
    "ax2.set_title('Actual Species')\n",
    "plt.colorbar(scatter2, ax=ax2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Explained variance by 2 PCA components: {pca.explained_variance_ratio_.sum():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster characteristics\n",
    "print(\"Cluster Characteristics (Mean values):\")\n",
    "print(\"=\" * 60)\n",
    "cluster_means = df.groupby('cluster')[feature_names].mean()\n",
    "print(cluster_means.round(2))\n",
    "\n",
    "print(\"\\nCluster Sizes:\")\n",
    "print(df['cluster'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Regression Analysis\n",
    "\n",
    "Let's predict one numeric variable (petal length) using the other numeric variables (sepal length, sepal width, and petal width)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for regression\n",
    "# Target: petal length\n",
    "# Features: sepal length, sepal width, petal width\n",
    "regression_features = ['sepal length (cm)', 'sepal width (cm)', 'petal width (cm)']\n",
    "X_reg = df[regression_features]\n",
    "y_reg = df['petal length (cm)']\n",
    "\n",
    "# Split the data\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
    "    X_reg, y_reg, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Regression task: Predicting Petal Length\")\n",
    "print(f\"Training samples: {len(X_train_reg)}\")\n",
    "print(f\"Testing samples: {len(X_test_reg)}\")\n",
    "print(f\"Features: {regression_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train multiple regression models\n",
    "regression_models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge Regression': Ridge(alpha=1.0),\n",
    "    'Lasso Regression': Lasso(alpha=0.1),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "regression_results = {}\n",
    "\n",
    "for name, model in regression_models.items():\n",
    "    # Train the model\n",
    "    model.fit(X_train_reg, y_train_reg)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_train = model.predict(X_train_reg)\n",
    "    y_pred_test = model.predict(X_test_reg)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_mse = mean_squared_error(y_train_reg, y_pred_train)\n",
    "    test_mse = mean_squared_error(y_test_reg, y_pred_test)\n",
    "    train_r2 = r2_score(y_train_reg, y_pred_train)\n",
    "    test_r2 = r2_score(y_test_reg, y_pred_test)\n",
    "    \n",
    "    regression_results[name] = {\n",
    "        'model': model,\n",
    "        'predictions': y_pred_test,\n",
    "        'train_mse': train_mse,\n",
    "        'test_mse': test_mse,\n",
    "        'train_r2': train_r2,\n",
    "        'test_r2': test_r2\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Train MSE: {train_mse:.4f}, Train R²: {train_r2:.4f}\")\n",
    "    print(f\"  Test MSE: {test_mse:.4f}, Test R²: {test_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize regression results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, (name, results) in enumerate(regression_results.items()):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Scatter plot of actual vs predicted\n",
    "    ax.scatter(y_test_reg, results['predictions'], alpha=0.6)\n",
    "    \n",
    "    # Perfect prediction line\n",
    "    min_val = min(y_test_reg.min(), results['predictions'].min())\n",
    "    max_val = max(y_test_reg.max(), results['predictions'].max())\n",
    "    ax.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2)\n",
    "    \n",
    "    ax.set_xlabel('Actual Petal Length (cm)')\n",
    "    ax.set_ylabel('Predicted Petal Length (cm)')\n",
    "    ax.set_title(f'{name}\\nTest R² = {results[\"test_r2\"]:.3f}')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance for Linear Regression\n",
    "linear_model = regression_results['Linear Regression']['model']\n",
    "coefficients = pd.DataFrame({\n",
    "    'Feature': regression_features,\n",
    "    'Coefficient': linear_model.coef_\n",
    "}).sort_values('Coefficient', key=abs, ascending=False)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.barh(coefficients['Feature'], coefficients['Coefficient'])\n",
    "plt.xlabel('Coefficient Value')\n",
    "plt.title('Linear Regression Feature Coefficients')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"Linear Regression Equation:\")\n",
    "print(f\"Petal Length = {linear_model.intercept_:.3f}\", end='')\n",
    "for feat, coef in zip(regression_features, linear_model.coef_):\n",
    "    print(f\" + ({coef:.3f} × {feat})\", end='')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual analysis\n",
    "best_model = 'Random Forest'  # Based on R² scores\n",
    "residuals = y_test_reg - regression_results[best_model]['predictions']\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Residual plot\n",
    "ax1.scatter(regression_results[best_model]['predictions'], residuals, alpha=0.6)\n",
    "ax1.axhline(y=0, color='r', linestyle='--')\n",
    "ax1.set_xlabel('Predicted Values')\n",
    "ax1.set_ylabel('Residuals')\n",
    "ax1.set_title(f'Residual Plot - {best_model}')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Histogram of residuals\n",
    "ax2.hist(residuals, bins=20, edgecolor='black', alpha=0.7)\n",
    "ax2.set_xlabel('Residuals')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.set_title('Distribution of Residuals')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Mean Residual: {residuals.mean():.4f}\")\n",
    "print(f\"Std Residual: {residuals.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Classification Analysis\n",
    "\n",
    "Now let's use various algorithms to classify iris species based on the four numeric features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for classification\n",
    "X_class = df[feature_names]\n",
    "y_class = df['species']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_class, y_class, test_size=0.3, random_state=42, stratify=y_class\n",
    ")\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Classification task: Predicting Iris Species\")\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Testing samples: {len(X_test)}\")\n",
    "print(f\"Classes: {target_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define classifiers\n",
    "classifiers = {\n",
    "    'SVM (Linear)': SVC(kernel='linear', random_state=42),\n",
    "    'SVM (RBF)': SVC(kernel='rbf', random_state=42),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=5),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=200, random_state=42)\n",
    "}\n",
    "\n",
    "# Train and evaluate each classifier\n",
    "classification_results = {}\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    # Use scaled data for algorithms that benefit from it\n",
    "    if name in ['SVM (Linear)', 'SVM (RBF)', 'KNN', 'Logistic Regression']:\n",
    "        clf.fit(X_train_scaled, y_train)\n",
    "        y_pred = clf.predict(X_test_scaled)\n",
    "        # Cross-validation\n",
    "        cv_scores = cross_val_score(clf, X_train_scaled, y_train, cv=5)\n",
    "    else:\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        # Cross-validation\n",
    "        cv_scores = cross_val_score(clf, X_train, y_train, cv=5)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    classification_results[name] = {\n",
    "        'model': clf,\n",
    "        'predictions': y_pred,\n",
    "        'accuracy': accuracy,\n",
    "        'cv_scores': cv_scores,\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std()\n",
    "    }\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Test Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  CV Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize classifier performance\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Accuracy comparison\n",
    "accuracies = [results['accuracy'] for results in classification_results.values()]\n",
    "names = list(classification_results.keys())\n",
    "\n",
    "ax1.barh(names, accuracies, color='skyblue', edgecolor='navy')\n",
    "ax1.set_xlabel('Accuracy')\n",
    "ax1.set_title('Test Accuracy by Classifier')\n",
    "ax1.set_xlim(0.8, 1.0)\n",
    "for i, v in enumerate(accuracies):\n",
    "    ax1.text(v + 0.005, i, f'{v:.3f}', va='center')\n",
    "\n",
    "# Cross-validation scores\n",
    "cv_means = [results['cv_mean'] for results in classification_results.values()]\n",
    "cv_stds = [results['cv_std'] for results in classification_results.values()]\n",
    "\n",
    "ax2.barh(names, cv_means, xerr=cv_stds, color='lightcoral', edgecolor='darkred', \n",
    "         capsize=5)\n",
    "ax2.set_xlabel('Cross-Validation Accuracy')\n",
    "ax2.set_title('5-Fold Cross-Validation Accuracy')\n",
    "ax2.set_xlim(0.8, 1.0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrices for top 3 classifiers\n",
    "top_classifiers = sorted(classification_results.items(), \n",
    "                        key=lambda x: x[1]['accuracy'], reverse=True)[:3]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for idx, (name, results) in enumerate(top_classifiers):\n",
    "    cm = confusion_matrix(y_test, results['predictions'])\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx],\n",
    "                xticklabels=target_names, yticklabels=target_names)\n",
    "    axes[idx].set_title(f'{name}\\nAccuracy: {results[\"accuracy\"]:.3f}')\n",
    "    axes[idx].set_ylabel('True Label')\n",
    "    axes[idx].set_xlabel('Predicted Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed classification report for the best classifier\n",
    "best_classifier = max(classification_results.items(), key=lambda x: x[1]['accuracy'])\n",
    "best_name, best_results = best_classifier\n",
    "\n",
    "print(f\"\\nDetailed Classification Report for {best_name}:\")\n",
    "print(\"=\" * 60)\n",
    "print(classification_report(y_test, best_results['predictions'], \n",
    "                          target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance for tree-based models\n",
    "tree_models = ['Decision Tree', 'Random Forest']\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "for idx, model_name in enumerate(tree_models):\n",
    "    if model_name in classification_results:\n",
    "        model = classification_results[model_name]['model']\n",
    "        importances = model.feature_importances_\n",
    "        \n",
    "        # Sort features by importance\n",
    "        indices = np.argsort(importances)[::-1]\n",
    "        \n",
    "        axes[idx].bar(range(len(importances)), importances[indices])\n",
    "        axes[idx].set_xticks(range(len(importances)))\n",
    "        axes[idx].set_xticklabels([feature_names[i] for i in indices], rotation=45, ha='right')\n",
    "        axes[idx].set_ylabel('Feature Importance')\n",
    "        axes[idx].set_title(f'Feature Importance - {model_name}')\n",
    "        axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for SVM\n",
    "print(\"Hyperparameter Tuning for SVM (RBF kernel)\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1]\n",
    "}\n",
    "\n",
    "svm = SVC(kernel='rbf', random_state=42)\n",
    "grid_search = GridSearchCV(svm, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation accuracy: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Test the best model\n",
    "best_svm = grid_search.best_estimator_\n",
    "y_pred_best = best_svm.predict(X_test_scaled)\n",
    "best_accuracy = accuracy_score(y_test, y_pred_best)\n",
    "print(f\"Test accuracy with best parameters: {best_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Comparison and Conclusions\n",
    "\n",
    "Let's summarize our findings from all three types of analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table\n",
    "summary_data = []\n",
    "\n",
    "# Clustering summary\n",
    "summary_data.append({\n",
    "    'Analysis Type': 'K-Means Clustering',\n",
    "    'Best Configuration': 'k=3',\n",
    "    'Performance Metric': f'Silhouette Score: {silhouette_score(X_scaled, cluster_labels):.3f}',\n",
    "    'Key Finding': 'Clusters align well with actual species'\n",
    "})\n",
    "\n",
    "# Regression summary\n",
    "best_reg = max(regression_results.items(), key=lambda x: x[1]['test_r2'])\n",
    "summary_data.append({\n",
    "    'Analysis Type': 'Regression (Petal Length)',\n",
    "    'Best Configuration': best_reg[0],\n",
    "    'Performance Metric': f'R² Score: {best_reg[1][\"test_r2\"]:.3f}',\n",
    "    'Key Finding': 'Petal width is the strongest predictor'\n",
    "})\n",
    "\n",
    "# Classification summary\n",
    "summary_data.append({\n",
    "    'Analysis Type': 'Classification (Species)',\n",
    "    'Best Configuration': best_name,\n",
    "    'Performance Metric': f'Accuracy: {best_results[\"accuracy\"]:.3f}',\n",
    "    'Key Finding': 'Multiple algorithms achieve perfect/near-perfect accuracy'\n",
    "})\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\nAnalysis Summary:\")\n",
    "print(\"=\" * 80)\n",
    "print(summary_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final visualization: 3D scatter plot\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure(figsize=(12, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Use the three most important features based on our analysis\n",
    "colors = ['blue', 'green', 'red']\n",
    "markers = ['o', '^', 's']\n",
    "\n",
    "for i, species in enumerate(target_names):\n",
    "    mask = df['species'] == i\n",
    "    ax.scatter(df.loc[mask, 'petal length (cm)'],\n",
    "              df.loc[mask, 'petal width (cm)'],\n",
    "              df.loc[mask, 'sepal length (cm)'],\n",
    "              c=colors[i], marker=markers[i], label=species, s=50)\n",
    "\n",
    "ax.set_xlabel('Petal Length (cm)')\n",
    "ax.set_ylabel('Petal Width (cm)')\n",
    "ax.set_zlabel('Sepal Length (cm)')\n",
    "ax.set_title('3D Visualization of Iris Dataset')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Data Characteristics**:\n",
    "   - The Iris dataset is well-balanced with 50 samples per species\n",
    "   - Petal measurements show stronger correlations and better species separation than sepal measurements\n",
    "   - No missing values or outliers requiring special treatment\n",
    "\n",
    "2. **K-Means Clustering**:\n",
    "   - Optimal number of clusters (k=3) matches the actual number of species\n",
    "   - Clusters align remarkably well with true species labels\n",
    "   - Setosa is perfectly separated, while Versicolor and Virginica show some overlap\n",
    "\n",
    "3. **Regression Analysis**:\n",
    "   - All models achieved high R² scores (>0.95) for predicting petal length\n",
    "   - Random Forest slightly outperformed linear models\n",
    "   - Petal width is the most important predictor of petal length\n",
    "\n",
    "4. **Classification**:\n",
    "   - Multiple algorithms achieved perfect or near-perfect accuracy\n",
    "   - SVM, Random Forest, and KNN were top performers\n",
    "   - The dataset is relatively easy to classify due to clear species separation\n",
    "\n",
    "### Recommendations:\n",
    "\n",
    "1. **For Species Identification**: Use any of the top-performing classifiers (SVM, Random Forest, or KNN)\n",
    "2. **For Understanding Relationships**: Linear models provide interpretable coefficients\n",
    "3. **For Feature Selection**: Focus on petal measurements for best results\n",
    "4. **For Production Use**: Consider ensemble methods for robustness\n",
    "\n",
    "The Iris dataset demonstrates that even simple machine learning algorithms can achieve excellent results when the data has clear patterns and good feature separation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai_coh2_3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
